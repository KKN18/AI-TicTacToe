# -*- coding: utf-8 -*-
"""TicTacToe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XyTagSi10v2P0H0kiiLOxM1GrqW--Gnp
"""

# !pip install gym

# Commented out IPython magic to ensure Python compatibility.
# !git clone https://github.com/ClementRomac/gym-tictactoe
# %cd gym-tictactoe
# !python setup.py install
import argparse
import gym
import gym_tictactoe
import random
import numpy as np
import torch
env = gym.make('TicTacToe-v1', symbols=[-1, 1], board_size=3, win_size=3)

parser = argparse.ArgumentParser()
parser.add_argument('--epochs', type=int, default=10, help='number of epochs of training')
parser.add_argument('--ckpt0_path', type=str, default='/content/drive/MyDrive/Colab Notebooks/Reinforcement Learning/us0_weight/', help='directory to save user0 weight')
parser.add_argument('--ckpt1_path', type=str, default='/content/drive/MyDrive/Colab Notebooks/Reinforcement Learning/us1_weight/', help='directory to save user1 weight')
opt = parser.parse_args()

if not torch.cuda.is_available():
    print("WARNING: You have to run with CUDA device")

def one_hot_encoding(action):
    len_action = 9
    one_hot_vector = [0]* len_action
    index = action
    one_hot_vector[index]=1
    return one_hot_vector

def randomAction(s, n=None):
    return random.randrange(0, 9)

def us0_data_preparation(N, K, f0, f1, render=False, extra_f0 = randomAction, extra_f1 = randomAction):
    game_data = []
    for i in range(N):
        game_steps = []
        state = env.reset()
        user = 0
        reward = 0
        done = False
        score = 0
        while not done:
            if render: env.render(mode=None)
            if user == 0:
                us0_action = f0(state)
                nTry = 2
                while state[us0_action] != 0:
                    us0_action = extra_f0(state, nTry)
                    nTry += 1
                game_steps.append((state, us0_action))
                state, reward, done, info = env.step(us0_action, -1)
            elif user == 1:
                us1_action = f1(state)
                nTry = 2
                while state[us1_action] != 0:
                    us1_action = extra_f1(state, nTry)
                    nTry += 1
                _, _, done, info = env.step(us1_action, 1)
            
            if not done:
                user = 0 if user == 1 else 1
            else:
                if reward == 10:
#                     print("Draw !")
                    score = reward
                elif reward == -20:
                    if user == 0:
                        score = reward
#                         print("Random wins ! AI Reward : " + str(reward))
                    elif user == 1:
                        score = -reward
#                         print("AI wins ! AI Reward : " + str(-reward))
                elif reward == 20:
                    if user == 0:
                        score = reward
#                         print("AI wins ! AI Reward : " + str(reward))
                    elif user == 1:
                        score = -reward
#                         print("Random wins ! AI Reward : " + str(reward))
        game_data.append((score, game_steps))
    game_data.sort(key=lambda s:-s[0])
    
    training_set = []
    for i in range(K):
        for step in game_data[i][1]:
            training_set.append((step[0], one_hot_encoding(step[1])))
    print("Complete")
    if render:
        for i in game_data:
            print("Score: {0}".format(i[0]))
    return training_set

def us1_data_preparation(N, K, f0, f1, render=False, extra_f0 = randomAction, extra_f1 = randomAction):
    game_data = []
    for i in range(N):
        game_steps = []
        state = env.reset()
        user = 0
        reward = 0
        done = False
        score = 0
        while not done:
            if render: env.render(mode=None)
            if user == 0:
                us0_action = f0(state)
                nTry = 2
                while state[us0_action] != 0:
                    us0_action = extra_f0(state, nTry)
                    nTry += 1
                _, _, done, info = env.step(us0_action, -1)
            elif user == 1:
                us1_action = f1(state)
                nTry = 2
                while state[us1_action] != 0:
                    us1_action = extra_f1(state, nTry)
                    nTry += 1
                game_steps.append((state, us1_action))
                state, reward, done, info = env.step(us1_action, 1)

            
            if not done:
                user = 0 if user == 1 else 1
            else:
                if reward == 10:
#                     print("Draw !")
                    score = reward
                elif reward == -20:
                    if user == 0:
                        score = -reward
#                         print("Random wins ! AI Reward : " + str(reward))
                    elif user == 1:
                        score = reward
#                         print("AI wins ! AI Reward : " + str(-reward))
                elif reward == 20:
                    if user == 0:
                        score = -reward
#                         print("AI wins ! AI Reward : " + str(reward))
                    elif user == 1:
                        score = reward
#                         print("Random wins ! AI Reward : " + str(reward))

        game_data.append((score, game_steps))
    game_data.sort(key=lambda s:-s[0])
    
    training_set = []
    for i in range(K):
        for step in game_data[i][1]:
            training_set.append((step[0], one_hot_encoding(step[1])))
    print("Complete")
    if render:
        for i in game_data:
            print("Score: {0}".format(i[0]))
    return training_set

us0_training_data= us0_data_preparation(1000, 50, randomAction, randomAction)

us1_training_data= us1_data_preparation(1000, 50, randomAction, randomAction)

import tensorflow as tf
from tensorflow.keras import Sequential, layers
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

def build_model():
    model = Sequential()
    model.add(Dense(128, input_dim=9, activation='relu'))
    model.add(Dense(52, activation='relu'))
    model.add(Dense(9, activation='softmax'))
    model.compile(loss='mse', optimizer=Adam())
    return model

def train_model(model, training_set):
    X = np.array([i[0] for i in training_set]).reshape(-1, 9)
    y = np.array([i[1] for i in training_set]).reshape(-1, 9)
    model.fit(X, y, opt.epoch)

def predict_or_random():
    res = random.randint(1, 10)
    if res % 3 != 0:
        return True
    else:
        return False

def us0_predictor(s):
    if predict_or_random():
        return np.argmax(us0_model.predict(np.array(s).reshape(-1, 9))[0])
    else:
        return random.randint(0, 8)

def us1_predictor(s):
    if predict_or_random():
        return np.argmax(us1_model.predict(np.array(s).reshape(-1, 9))[0])
    else:
        return random.randint(0, 8)

def extra_us0_predictor(s, n):
    lst = us0_model.predict(np.array(s).reshape(-1, 9))[0]
    for i in range(n-1):
        iMax = np.argmax(lst)
        lst[iMax] = -1

    if predict_or_random():
        return np.argmax(lst)
    else:
        ret = random.randint(0, 8)
        while s[ret] != 0:
            ret = random.randint(0, 8)
        return ret

def extra_us1_predictor(s, n):
    lst = us1_model.predict(np.array(s).reshape(-1, 9))[0]
    for i in range(n-1):
        iMax = np.argmax(lst)
        lst[iMax] = -1
    
    if predict_or_random():
        return np.argmax(lst)
    else:
        ret = random.randint(0, 8)
        while s[ret] != 0:
            ret = random.randint(0, 8)
        return ret

with tf.device("gpu:0"):
    N = 1000
    K = 50
    us0_model = build_model()
    train_model(us0_model, us0_training_data)
    us1_model = build_model()
    train_model(us1_model, us1_training_data)
    self_play_count = 20

    for i in range(self_play_count):
        print("Self playing count : " + str(i))
        K = (N//9 + K)//2
        us0_training_data = us0_data_preparation(N, K, us0_predictor, us1_predictor, extra_f0=extra_us0_predictor, extra_f1=extra_us1_predictor)
        us1_training_data = us1_data_preparation(N, K, us0_predictor, us1_predictor, extra_f0=extra_us0_predictor, extra_f1=extra_us1_predictor)
        train_model(us0_model, us0_training_data)
        train_model(us1_model, us1_training_data)
        print("Complete!")

# !pip install -q pyyaml h5py

checkpoint_path0 = f"{opt.ckpt0_path}/us0.ckpt"
us0_model.save_weights(checkpoint_path0)

checkpoint_path1 = f"{opt.ckpt1_path}/us1.ckpt"
us1_model.save_weights(checkpoint_path1)